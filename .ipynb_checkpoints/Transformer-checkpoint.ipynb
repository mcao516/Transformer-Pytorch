{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Implement the position embedding function.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()      \n",
    "        # Compute the positional encodings once in log space\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # position: [max_len, 1]\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        # div_term: [d_model/2]\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * \n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"Inplement Embedding layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab_size, dropout=0.1):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embed = PositionalEncoding(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        e = self.embed(x) * math.sqrt(self.d_model)\n",
    "        e = self.position_embed(e)\n",
    "        return self.dropout(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Implement one encoder layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, attn, norm, feed_forward):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attn = attn\n",
    "        self.norm = norm\n",
    "        self.feed_forward = feed_forward\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Forward through one encoder layer: multi-head attn => add & norm\n",
    "           => feed forward => add & norm.\n",
    "        \n",
    "        Args:\n",
    "            x: embeddings or output of the last layer.\n",
    "                [batch_size, seq_len, d_model]\n",
    "            mask: [batch_size, (1 or seq_len), seq_len]\n",
    "        \"\"\"\n",
    "        # multihead attn & norm\n",
    "        a = self.attn(x, x, x, mask)\n",
    "        t = self.norm(x + a)\n",
    "        \n",
    "        # feed forward & norm\n",
    "        z = self.feed_forward(t)\n",
    "        y = self.norm(t + z)\n",
    "        \n",
    "        assert x.shape == y.shape\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentioin(nn.Module):\n",
    "    \"\"\"Implement a multi-head attention layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, head_num, dropout=0.1):\n",
    "        super(MultiHeadAttentioin, self).__init__()\n",
    "        # multi-head attention\n",
    "        assert d_model % head_num == 0\n",
    "        \n",
    "        self.d_model  = d_model\n",
    "        self.head_num = head_num\n",
    "        self.d_k = d_model // head_num\n",
    "        \n",
    "        # d_model = d_k * head_num\n",
    "        self.W_Q = nn.Linear(d_model, head_num * self.d_k)\n",
    "        self.W_K = nn.Linear(d_model, head_num * self.d_k)\n",
    "        self.W_V = nn.Linear(d_model, head_num * self.d_k)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def scaled_dp_attn(self, query, key, value, mask=None):\n",
    "        \"\"\"Compute Scaled Dot-Product Attention function.\n",
    "        \n",
    "        Args:\n",
    "            query: [batch_size, head_num, seq_len, d_k]\n",
    "            key:   [batch_size, head_num, seq_len, d_k]\n",
    "            value: [batch_size, head_num, seq_len, d_k]\n",
    "            mask:  [batch_size, (1 or seq_len), seq_len]\n",
    "        \"\"\"\n",
    "        assert self.d_k == query.shape[-1]\n",
    "        \n",
    "        # scores: [batch_size, head_num, seq_len, seq_len]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.d_k\n",
    "        if mask is not None:\n",
    "            assert len(mask.shape) == 3\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # attn: [batch_size, head_num, seq_len, seq_len]\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        return torch.matmul(attn, value), attn\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        \"\"\"First linearly proj the queries, keys and values, then apply\n",
    "           scaled dot-product attention.\n",
    "           \n",
    "        Args:\n",
    "            q: embeddings or output of the last layer.\n",
    "                [batch_size, seq_len, d_model]\n",
    "            mask: [batch_size, (1 or seq_len), seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        # query, key, value: [batch_size, head_num, seq_len, d_k]\n",
    "        query = self.W_Q(q).view(batch_size, -1, self.head_num, self.d_k).transpose(1, 2)\n",
    "        key   = self.W_K(k).view(batch_size, -1, self.head_num, self.d_k).transpose(1, 2)\n",
    "        value = self.W_V(v).view(batch_size, -1, self.head_num, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # attn: [batch_size, head_num, seq_len, seq_len]\n",
    "        heads, attn = self.scaled_dp_attn(query, key, value, mask)\n",
    "        heads = heads.transpose(1, 2).contiguous().view(batch_size, -1, \n",
    "                                                        self.head_num * self.d_k)\n",
    "        # heads: [batch_size, seq_len, d_model]\n",
    "        assert heads.shape[-1] == self.d_model \n",
    "        assert heads.shape[0]  == batch_size\n",
    "        \n",
    "        # Concat(head_1, ..., head_n)W_O\n",
    "        y = self.W_O(heads)\n",
    "        assert y.shape == q.shape\n",
    "        return self.dropout(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Construct a layernorm module.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_size, e=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.g = nn.Parameter(torch.ones(layer_size))\n",
    "        self.b = nn.Parameter(torch.zeros(layer_size))\n",
    "        self.e = e\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std  = x.std(-1, keepdim=True)\n",
    "        x = (x - mean) / (std + self.e)\n",
    "        return self.g * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Implement a feed-forward neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, act='relu'):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        self.ffn_1 = nn.Linear(d_model, d_ff)\n",
    "        self.ffn_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if act == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif act == 'rrelu':\n",
    "            self.act = nn.RReLU()\n",
    "        else:\n",
    "            self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        h = self.act(self.ffn_1(x))\n",
    "        y = self.ffn_2(h)\n",
    "        return self.dropout(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"Clone N identical layers.\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The encoder is composed of a stack of N = 6 identical layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, N, head_num, d_ff, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.layers = clones(EncoderLayer(MultiHeadAttentioin(d_model, head_num, dropout=dropout), \n",
    "                                          LayerNorm(d_model), \n",
    "                                          FeedForward(d_model, d_ff, dropout=dropout)), N)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Forward through N identical layers.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "            mask: [batch_size, 1, seq_len]\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Implement one decoder layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, self_attn, ed_attn, feed_forward, norm):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.ed_attn = ed_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.norm = norm\n",
    "        \n",
    "    def forward(self, x, mask, memory, src_mask):\n",
    "        \"\"\"Forward through a decoder: self attn & norm => \n",
    "           encoder-decoder attn & norm => feed forward & norm.\n",
    "        \"\"\"\n",
    "        # self-attn & norm\n",
    "        a_1 = self.self_attn(x, x, x, mask)\n",
    "        t_1 = self.norm(x + a_1)\n",
    "        \n",
    "        # encoder-decoder attn & norm\n",
    "        a_2 = self.ed_attn(t_1, memory, memory, src_mask)\n",
    "        t_2 = self.norm(t_1 + a_2)\n",
    "        \n",
    "        # feed forward & norm\n",
    "        h = self.feed_forward(t_2)\n",
    "        y = self.norm(t_2 + h)\n",
    "        \n",
    "        assert x.shape == y.shape\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Implement Transformer decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, N, head_num, d_ff, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(DecoderLayer(MultiHeadAttentioin(d_model, head_num, dropout=dropout), \n",
    "                                          MultiHeadAttentioin(d_model, head_num, dropout=dropout), \n",
    "                                          FeedForward(d_model, d_ff, dropout=dropout),\n",
    "                                          LayerNorm(d_model)), N)\n",
    "    \n",
    "    def forward(self, x, mask, memory, src_mask):\n",
    "        \"\"\"Forward through N identical decoder layers.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "            mask: [batch_size, seq_len, seq_len]\n",
    "            memory: [batch_size, seq_len, d_model]\n",
    "            src_mask: [batch_size, 1, d_model]\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask, memory, src_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample test with toy data\n",
    "d_model, N, head_num, d_ff, vocab_size = 512, 3, 8, 512*4, 10\n",
    "\n",
    "embed_layer = EmbeddingLayer(d_model, vocab_size)\n",
    "encoder = Encoder(d_model, N, head_num, d_ff)\n",
    "decoder = Decoder(d_model, N, head_num, d_ff)\n",
    "\n",
    "test_input = torch.tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])\n",
    "en_mask  = torch.ones(test_input.shape[0], 1, 5)\n",
    "de_mask  = torch.ones(test_input.shape[0], 5, 5)\n",
    "\n",
    "src_output = encoder(embed_layer(test_input), en_mask)\n",
    "output = decoder(embed_layer(test_input), de_mask, src_output, en_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n",
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "print(src_output.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSoftmax(nn.Module):\n",
    "    \"\"\"Implement the final linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(LinearSoftmax, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab = vocab\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        \n",
    "    def forward(self, x, prob=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           x: [batch_size, seq_len, d_model]\n",
    "           prob: if calculate probabilities.\n",
    "        \"\"\"\n",
    "        logits = self.proj(x)\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask(base_mask):\n",
    "    \"\"\"Build a mask for the Transformer decoder to mask all the\n",
    "       subsequent tokens.\n",
    "    \n",
    "    Args:\n",
    "       base_mask: basic mask for padded tokens. [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    assert len(base_mask.shape) == 2\n",
    "    batch_size, seq_len = base_mask.shape[0], base_mask.shape[-1]\n",
    "    \n",
    "    # create subsequent token mask\n",
    "    sub_mask = torch.tril(torch.ones([seq_len, seq_len], \n",
    "                                     dtype=torch.uint8)).type_as(base_mask)\n",
    "    sub_mask = sub_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    base_mask = base_mask.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "    return sub_mask & base_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"Implement an encoder-decoder architecture. \n",
    "    \"\"\"\n",
    "    def __init__(self, src_embed, tgt_embed, encoder, decoder, linear_softmax):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.linear_softmax = linear_softmax\n",
    "        \n",
    "    def forward(self, en_input, en_mask, de_input, de_mask):\n",
    "        \"\"\"Forward through the whole encoding & decoing process:\n",
    "           token embedding => position embedding => encoding =>\n",
    "           decoding => linear & softmax => probs\n",
    "           \n",
    "        Args:\n",
    "           en_input: source tokens. [batch_size, en_seq_len]\n",
    "           en_mask: source mask. [batch_size, en_seq_len]\n",
    "           de_input: target tokens. [batch_size, de_seq_len]\n",
    "           de_mask: target mask. [batch_size, de_seq_len]\n",
    "        \"\"\"\n",
    "        # build masks\n",
    "        en_mask = en_mask.unsqueeze(1)\n",
    "        de_mask = build_mask(de_mask)\n",
    "        \n",
    "        # token & position embedding\n",
    "        en_embeddings = self.src_embed(en_input)\n",
    "        de_embeddings = self.tgt_embed(de_input)\n",
    "        \n",
    "        # encoding & decoding\n",
    "        en_output = self.encoder(en_embeddings, en_mask)\n",
    "        de_output = self.decoder(de_embeddings, de_mask, en_output, en_mask)\n",
    "        \n",
    "        # linear & softmax\n",
    "        probs = self.linear_softmax(de_output)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "d_model, N, head_num, d_ff, vocab_size = 512, 3, 8, 512*4, 11\n",
    "\n",
    "# modules\n",
    "src_embed = EmbeddingLayer(d_model, vocab_size)\n",
    "tgt_embed = src_embed\n",
    "encoder = Encoder(d_model, N, head_num, d_ff)\n",
    "decoder = Decoder(d_model, N, head_num, d_ff)\n",
    "linear_softmax = LinearSoftmax(d_model, vocab_size)\n",
    "\n",
    "encoder_decoder = EncoderDecoder(src_embed, tgt_embed, encoder, decoder, linear_softmax)\n",
    "\n",
    "# toy data\n",
    "test_input = torch.tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])\n",
    "en_mask = torch.ones([test_input.shape[0], 5], dtype=torch.uint8)\n",
    "de_mask = torch.ones([test_input.shape[0], 5], dtype=torch.uint8)\n",
    "\n",
    "# forward\n",
    "probs = encoder_decoder(test_input, en_mask, test_input, de_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 11])\n"
     ]
    }
   ],
   "source": [
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss - Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"Implement label smoothing loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        \"\"\"Compute loss with label smoothing: create a new distribution\n",
    "           that has confidence of the correct word and smoothing distributed \n",
    "           throughout the rest of the vocabulary.\n",
    "        \n",
    "        Args:\n",
    "            x: reshaped softmax output. \n",
    "                [batch_size * target_seq_len, vocab]\n",
    "            target: reshaped target labels.\n",
    "                [batch_size * target_seq_len]\n",
    "        \"\"\"\n",
    "        vocab_size = x.shape[1]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
